{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fbba1f",
   "metadata": {},
   "source": [
    " Design RNN or its variant including LSTM or GRU a) Select a suitable time series dataset.      \n",
    "Example – predict sentiments based on product reviews b) Apply for prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f466cd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 384ms/step - accuracy: 0.6171 - loss: 0.6278 - val_accuracy: 0.8380 - val_loss: 0.3714\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 384ms/step - accuracy: 0.8728 - loss: 0.3155 - val_accuracy: 0.8420 - val_loss: 0.3918\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 388ms/step - accuracy: 0.8924 - loss: 0.2798 - val_accuracy: 0.8740 - val_loss: 0.3069\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 409ms/step - accuracy: 0.9135 - loss: 0.2279 - val_accuracy: 0.8740 - val_loss: 0.3297\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 411ms/step - accuracy: 0.9260 - loss: 0.1934 - val_accuracy: 0.8728 - val_loss: 0.3112\n",
      "Test Accuracy: 87.02%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n            Explanation:\\n            evaluate(): Computes loss and accuracy on unseen test data.\\n            test_acc: Accuracy reflects how well the model generalizes to new reviews.\\n            Typical Output: ~80-88% accuracy depending on hyperparameters.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Explanation:\n",
    "\n",
    "\"\"\" \n",
    "            Sequential: A linear stack of layers to build the model.\n",
    "            Embedding: Converts integer-encoded words into dense vectors (e.g., \"cat\" → [0.2, -0.5, ...]).\n",
    "            LSTM: Layer to process sequential data with memory cells and gates.\n",
    "            Dense: Fully connected layer for classification.\n",
    "            imdb: Preloaded dataset of movie reviews labeled as positive (1) or negative (0).\n",
    "            sequence: Utilities for padding sequences to a fixed length. \n",
    "\"\"\"\n",
    "\n",
    "# Load dataset\n",
    "vocab_size = 5000  # Use top 5,000 frequent words\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "\"\"\"\n",
    "            Explanation:\n",
    "            vocab_size=5000: Restrict vocabulary to the 5,000 most frequent words (reduces noise from rare words).\n",
    "            imdb.load_data(): Loads the IMDB dataset preprocessed into integer sequences.\n",
    "            x_train/x_test: Lists of reviews, where each word is replaced by its integer index.\n",
    "            y_train/y_test: Labels (0 or 1).\n",
    "\"\"\"\n",
    "\n",
    "# Pad sequences to fixed length (400 words)\n",
    "max_words = 400\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)\n",
    "\"\"\"\n",
    "            Explanation:\n",
    "            max_words=400: Truncate/pad all reviews to 400 words.\n",
    "            Shorter reviews are padded with zeros (e.g., [0, 0, ..., 12, 42]).\n",
    "            Longer reviews are truncated to 400 words.\n",
    "            Why? Neural networks require fixed-length inputs for batch processing.\n",
    "\"\"\"\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential(name=\"LSTM_Sentiment_Analysis\")\n",
    "model.add(Embedding(vocab_size, 32, input_length=max_words))  # Convert word indices to 32D vectors\n",
    "model.add(LSTM(128, activation='tanh', return_sequences=False))  # 128 LSTM units\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\"\"\"\n",
    "            Step 1: Embedding Layer:\n",
    "            vocab_size: 5,000 unique words.\n",
    "            32: Embedding dimension (each word is a 32-dimensional vector).\n",
    "            input_length=max_words: Each input sequence has 400 words.\n",
    "            Purpose: Converts sparse integer-encoded words into dense vectors that capture semantic meaning (e.g., \"good\" and \"great\" are closer in vector space).\n",
    "\n",
    "            Step 2: LSTM Layer:\n",
    "            128: Number of LSTM units (dimensionality of the hidden state).\n",
    "            activation='tanh': Hyperbolic tangent activation for gate updates.\n",
    "            return_sequences=False: Return only the final output (not all timesteps).\n",
    "            Purpose: Processes the sequence word-by-word, updating its hidden state to capture context.\n",
    "\n",
    "            Step 3: Dense Layer:\n",
    "            1: Single neuron for binary classification (positive/negative).\n",
    "            activation='sigmoid': Squashes output to [0, 1] (probability of positive sentiment).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\"\"\"\n",
    "            Explanation:\n",
    "            loss='binary_crossentropy': Standard loss for binary classification.\n",
    "            optimizer='adam': Adaptive learning rate optimizer (efficient for RNNs).\n",
    "            metrics=['accuracy']: Track accuracy during training.\n",
    "\"\"\"\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x_train, y_train, \n",
    "                   batch_size=64, \n",
    "                   epochs=5, \n",
    "                   validation_split=0.2)\n",
    "\"\"\"\n",
    "            Explanation:\n",
    "            batch_size=64: Update weights after every 64 samples (balance speed/memory).\n",
    "            epochs=5: Train for 5 full passes over the training data.\n",
    "            validation_split=0.2: Use 20% of training data for validation (monitor overfitting).\n",
    "            Output: Training logs show loss/accuracy for training and validation sets.\n",
    "\"\"\"\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "            Explanation:\n",
    "            evaluate(): Computes loss and accuracy on unseen test data.\n",
    "            test_acc: Accuracy reflects how well the model generalizes to new reviews.\n",
    "            Typical Output: ~80-88% accuracy depending on hyperparameters.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16655163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 141ms/step - accuracy: 0.7133 - loss: 0.5177 - val_accuracy: 0.8707 - val_loss: 0.3086\n",
      "Epoch 2/3\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 172ms/step - accuracy: 0.9054 - loss: 0.2469 - val_accuracy: 0.8605 - val_loss: 0.3229\n",
      "Epoch 3/3\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 171ms/step - accuracy: 0.9377 - loss: 0.1745 - val_accuracy: 0.8684 - val_loss: 0.3592\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.8669 - loss: 0.3611\n",
      "Test Accuracy: 86.84%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Load the IMDb dataset\n",
    "vocab_size = 10000  # Only top 10,000 words\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "max_length = 200\n",
    "x_train = pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = pad_sequences(x_test, maxlen=max_length)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 64, input_length=max_length),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')  # Binary sentiment classification\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c886a0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
